{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/Users/mallicka/Documents/aadildev/personal/ai/pythonai/utils/langchain_rag.py:20: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  return OllamaEmbeddings(model=model)\n"
     ]
    }
   ],
   "source": [
    "from utils.langchain_rag import LangchainRAG, ChromaVectorStore, FAISSVectorStore, DocumentLoaders\n",
    "\n",
    "embeddings = LangchainRAG.get_OllamaEmbeddings()\n",
    "langchain_rag = LangchainRAG(embeddings)\n",
    "loaders = DocumentLoaders()\n",
    "chromaManager = ChromaVectorStore(path=\"chroma_db\", embeddings=embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mallicka/Documents/aadildev/personal/ai/pythonai/utils/langchain_rag.py:66: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunks = loaders.get_chunks_from_video(\"https://www.youtube.com/watch?v=OrliU0e09io\")\n",
    "chroma = chromaManager.create_chroma_db_from_texts(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lx/vf9brsnx5ns0t5llrz3lqf8h0000gp/T/ipykernel_72222/3101596220.py:15: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llama = Ollama(model=\"llama3.2\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "llama = Ollama(model=\"llama3.2\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "\n",
    "chain = prompt | llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I would say that React Query's popularity can be attributed to its ability to solve two fundamental problems in building React applications:\n",
      "\n",
      "1. The \"5:00 rule\" problem: As mentioned in the video, developers often get stuck thinking about how to handle data fetching and management, leading to long hours and frustration. React Query solves this by abstracting away the complexity of data fetching and caching, allowing developers to focus on building their app's UI.\n",
      "2. The \"async State\" problem: Managing server-side state in React applications is notoriously difficult due to the need for synchronization and caching across multiple components. React Query addresses this challenge by providing a centralized, reactive way to manage async state, making it easier for developers to build scalable and predictable applications.\n",
      "\n",
      "By solving these two critical problems, React Query has become an essential tool for many React developers, particularly those working on data-driven applications. Its popularity can be seen in the fact that it's used in one out of every six React applications, with over 3.3 million downloads per week.\n"
     ]
    }
   ],
   "source": [
    "def ragprompt(query: str):\n",
    "    docsContext = langchain_rag.get_related_docs_from_query(chroma, query)\n",
    "\n",
    "    result = chain.invoke({\"context\": docsContext, \"question\": \"Why is react query so popular?\"})\n",
    "\n",
    "    print(result)\n",
    "\n",
    "ragprompt(\"What is the main topic of the video?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ollama run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llama = ChatOllama(model=\"llama3.2\",temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected to MCP server\n",
      "tools length: 8\n",
      "available tools: ['firecrawl_scrape', 'firecrawl_map', 'firecrawl_crawl', 'firecrawl_check_crawl_status', 'firecrawl_search', 'firecrawl_extract', 'firecrawl_deep_research', 'firecrawl_generate_llmstxt']\n",
      "The provided code snippet is a collection of CSS recipes for modern web design, covering various topics such as UI trends, scrolling, and layout. Here's a breakdown of the different sections:\n",
      "\n",
      "1. **UI Trends**:\n",
      "\t* Glassmorphism: A style that combines a translucent background with a blurred overlay to create a sense of depth.\n",
      "\t* Neumorphism: A style that uses realistic shadows to create a soft, 3D-like effect.\n",
      "2. **Scrolling**:\n",
      "\t* Styling scrollbars: An example of how to customize the appearance of scrollbars using CSS.\n",
      "\t* Scroll snap: A feature that allows elements to snap into place when scrolling, creating a smooth and seamless experience.\n",
      "3. **Layouts**:\n",
      "\t* Centering: A method for centering elements horizontally or vertically using CSS grid.\n",
      "\t* Flex wrap equal size boxes: A technique for wrapping flex items in a flexible container, ensuring they take up equal space.\n",
      "\t* Auto size header and footer: A way to automatically adjust the height of a header or footer element based on its content.\n",
      "4. **CSS Layouts**:\n",
      "\t* RAM (repeat, auto-fill, minmax): A method for creating responsive layouts using CSS grid, which allows for flexible sizing and spacing.\n",
      "5. **Aspect Ratio**:\n",
      "\t* A technique for maintaining the aspect ratio of an element while resizing it.\n",
      "\n",
      "Some common techniques and properties used throughout these recipes include:\n",
      "\n",
      "* Using CSS grid to create responsive and flexible layouts\n",
      "* Utilizing CSS variables (e.g., `--basis-width`) to define reusable values\n",
      "* Leveraging CSS filters (e.g., `backdrop-filter: blur()`) to add visual effects\n",
      "* Employing CSS transitions and animations to enhance user experience\n",
      "\n",
      "To implement these recipes, you can follow the provided code snippets and adapt them to your specific use case. Keep in mind that some of these techniques may require additional setup or configuration, such as setting up a CSS grid container or defining custom variables.\n",
      "\n",
      "Here's an example of how you could apply some of these techniques to create a simple layout:\n",
      "```css\n",
      "/* Create a responsive grid container */\n",
      ".container {\n",
      "  display: grid;\n",
      "  grid-template-columns: repeat(auto-fill, minmax(200px, 1fr));\n",
      "  height: 100vh;\n",
      "}\n",
      "\n",
      "/* Center the content horizontally and vertically */\n",
      ".content {\n",
      "  display: flex;\n",
      "  justify-content: center;\n",
      "  align-items: center;\n",
      "  background-color: #f0f0f0;\n",
      "}\n",
      "```\n",
      "This code creates a responsive grid container with a minimum width of 200px, and automatically adjusts the number of columns based on the available space. The `content` element is centered horizontally and vertically using flexbox.\n",
      "\n",
      "Remember to experiment and adapt these recipes to your specific design needs and requirements.\n",
      "Connected to MCP server\n"
     ]
    }
   ],
   "source": [
    "# setup firecrawl mcp server\n",
    "\n",
    "from mcpUtils import MCPClient\n",
    "import asyncio\n",
    "from mcp import ClientSession\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# uti;lity for converting Tool instances into normal tool functions.\n",
    "from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "\n",
    "\n",
    "firecrawl_api_key = os.getenv(\"FIRECRAWL_API_KEY\")\n",
    "if not firecrawl_api_key:\n",
    "    raise ValueError(\"FIRECRAWL_API_KEY is not set\")\n",
    "\n",
    "stdioParams = MCPClient.get_stdio_server_params(\"npx\", [\"firecrawl-mcp\"], env={\"FIRECRAWL_API_KEY\": firecrawl_api_key})\n",
    "\n",
    "mcpClient = MCPClient((\"stdio\", stdioParams))\n",
    "\n",
    "async def on_connection_established(session: ClientSession):\n",
    "    print(\"connected to MCP server\")\n",
    "    tools = await load_mcp_tools(session)\n",
    "    print(f\"tools length: {len(tools)}\")\n",
    "    print(f\"available tools: {list(map(lambda tool: tool.name, tools))}\")\n",
    "    agent = create_react_agent(model=llama, tools=tools)\n",
    "\n",
    "    link = \"https://threejsnotes.netlify.app/docs/css-recipes/Modern-web-design\"\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant that can answer questions about the web.\"), \n",
    "        HumanMessage(content=f\"scrape the following link: {link} and then from that content, tell me how to implement glassmorphism in css\")\n",
    "    ]\n",
    "    result = await agent.ainvoke({\n",
    "        \"messages\": messages\n",
    "    })\n",
    "    messages = result[\"messages\"]\n",
    "    aiMessage = messages[-1].content\n",
    "    print(aiMessage)\n",
    "\n",
    "mcpClient.on_connection_established(on_connection_established)\n",
    "\n",
    "await mcpClient.connect_to_mcp_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
